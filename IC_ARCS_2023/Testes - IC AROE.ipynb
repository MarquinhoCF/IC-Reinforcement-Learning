{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Resolução de Problemas de Otimização sob Incerteza em Cadeias de Suprimentos através de Técnicas de Aprendizado por Reforço e Otimização Estocástica**\n",
    "\n",
    "## **Iniciação Científca - CNPq**\n",
    "\n",
    "### **Membros do Projeto**\n",
    "\n",
    "* Julio César Alves\n",
    "\n",
    "* Dilson Lucas Pereira\n",
    "\n",
    "* Marcos Carvalho Ferreira\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pip installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gymnasium in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (1.22.4)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (4.5.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting gym-walk\n",
      "  Cloning https://github.com/mimoralea/gym-walk to c:\\users\\user\\appdata\\local\\temp\\pip-install-anexzlws\\gym-walk_1bb1c6ddd13945ba9f0bfcecd474c7b2\n",
      "  Resolved https://github.com/mimoralea/gym-walk to commit 5999016267d6de2f5a63307fb00dfd63de319ac1\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: gym in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym-walk) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym->gym-walk) (1.22.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym->gym-walk) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym->gym-walk) (0.0.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/mimoralea/gym-walk 'C:\\Users\\User\\AppData\\Local\\Temp\\pip-install-anexzlws\\gym-walk_1bb1c6ddd13945ba9f0bfcecd474c7b2'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm\n",
    "%pip install gymnasium\n",
    "%pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk\n",
    "%pip install tabulate\n",
    "# %pip install git+https://github.com/ibm-cds-labs/pixiedust\n",
    "# %pip install -e ./pixiedust"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atenção**: após as instalações é recomendável **reiniciar o Kernel**. No VS Code, acesse a paleta de comandos (teclando `F1` OU `Ctrl+Shift+P`) e procurando a opção *Notebook: Restart Kernel* (dica: digite *kernel* na paleta para filtrar os comandos)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Você pode usar a biblioteca externa do Python tqdm , para criar barras de progresso simples e sem complicações\n",
    "# que você pode adicionar ao seu código e torná-lo mais animado!\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Como implementar? --> Ex: \"for e in tqdm(range(n_episodios), desc='Episodes for: ' + nome(string), leave=False):\"\n",
    "\n",
    "# tqdm(\"Intervalo desejado\", \"Descrição (desc)\", \"Leave\" Se [padrão: True], mantém\n",
    "# todos os traços da barra de progresso após o término da iteração):\n",
    "\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import gym_walk\n",
    "import gym as old_gym\n",
    "import numpy as np\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEMENTES = (5, 80, 78, 27, 17)\n",
    "# SEMENTES = (12, 34, 56, 78, 90) # usadas pelo autor do livro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funções úteis**\n",
    "\n",
    "* **Funções utilizadas nas estratégias:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função utilizada para decair alfa e epsilon exponencialmente.\n",
    "# Você dá um valor inicial, um valor mínimo e a porcentagem de max_iteracoes para decair os valores de inicial para mínimo.\n",
    "def decaimento_exponencial(valor_ini, valor_min, taxa_decaimento, max_iteracoes, log_inicio=-2, log_base=10):\n",
    "    # Índice onde o decaimento de valores termina e o valor_min vai até max_iteracoes.\n",
    "    decaimento_iteracoes = int(max_iteracoes * taxa_decaimento)\n",
    "\n",
    "    # Diferença entre o máximo de iterações e as iterações de decaimento\n",
    "    iteracoes_remanescentes = max_iteracoes - decaimento_iteracoes\n",
    "\n",
    "    # Usando o logspace para gerar uma escala logaritimica começando em log_start (padrão = -2), e terminando em 0. \n",
    "    # O número total de valores é decaimento_iteracoes e a base é log_base (padrão é 10). \n",
    "    # Perceba que está invertido com [::-1]!\n",
    "    valor = np.logspace(log_inicio, 0, decaimento_iteracoes, base=log_base, endpoint=True)[::-1]\n",
    "\n",
    "    # Os valores podem não terminar exatamente em 0, por causa do logaritmo\n",
    "    # Alteramos para ficar entre 0 e 1 e assim a curva fica suave e bonita\n",
    "    valor = (valor - valor.min()) / (valor.max() - valor.min())\n",
    "\n",
    "    # Fazemos a transformação linear entre valor_ini e valor_min.\n",
    "    valor = (valor_ini - valor_min) * valor + valor_min\n",
    "\n",
    "    # E então enchemos um array com \"valor\", a função \"pad\" repete o valor mais à direita iteracoes_remanescentes número de vezes.\n",
    "    valor = np.pad(valor, (0, iteracoes_remanescentes), 'edge')\n",
    "    \n",
    "    return valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Função utilizada para criar uma trajetória necessária para o controle de Monte Carlo\n",
    "\n",
    "# Esta versão do gerar_trajetoria é um pouco diferente. Agora precisamos levar em uma estratégia de seleção de ação, em vez de uma política gananciosa.\n",
    "\n",
    "# Você passa a estratégia de seleção de ação, a função Q, o epsilon (Quão gancioso o agente vai ser?), o ambiente e a quantidade maxima de iterações \n",
    "# (caso o número de iterações ultrapasse este número a trajetória é retornada vazia (truncar)).\n",
    "def gerar_trajetoria_epsilon_ganancioso(seleciona_acao, Q, epsilon, amb, max_iteracoes=200):\n",
    "    # Criamos a variavel bolleana \"terminado\" que somente se tornará True quando o resultado da iteração levar a um estado terminal\n",
    "    # Inicializamos o array que irá armazenar a trajetória (vazio)\n",
    "    terminado, trajetoria = False, []\n",
    "\n",
    "    # Loop principal\n",
    "    while not terminado:\n",
    "        # Resetamos o ambiente para garantir que ele esteja pronto para gerar a trajetória\n",
    "        estado = amb.reset()\n",
    "\n",
    "        # Entramos em loop infinito -> só para com break\n",
    "        for t in itertools.count():\n",
    "            # Selecionamos a acao para o estado atual de acordo com a estratégia passada\n",
    "            acao = seleciona_acao(estado, Q, epsilon)\n",
    "\n",
    "            # Realizamos uma iteração\n",
    "            prox_estado, recompensa, terminado, _ = amb.step(acao)\n",
    "            # Guardamos o resultado da iteração em um array\n",
    "            experiencia = (estado, acao, recompensa, prox_estado, terminado)\n",
    "            # append --> Acrescentamos a experiencia no final do array \"trajetória\"\n",
    "            trajetoria.append(experiencia)\n",
    "\n",
    "            # Para o loop quando chegamos em um estado terminal\n",
    "            if terminado:\n",
    "                break\n",
    "\n",
    "            # Truncamos a trajetória quando atinje o máximo de iterações\n",
    "            if t >= max_iteracoes - 1:\n",
    "                trajetoria = []\n",
    "                break\n",
    "\n",
    "            # Passamos para o proximo estado\n",
    "            estado = prox_estado\n",
    "            \n",
    "        return np.array(trajetoria, np.object)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Funções utilizadas para testes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imprima_funcao_valor_de_estado(V, P, n_cols=4, prec=3, title='Função valor de estado:'):\n",
    "    print(title)\n",
    "    for s in range(len(P)):\n",
    "        v = V[s]\n",
    "        print(\"| \", end=\"\")\n",
    "        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n",
    "            print(\"\".rjust(9), end=\" \")\n",
    "        else:\n",
    "            print(str(s).zfill(2), '{}'.format(np.round(v, prec)).rjust(6), end=\" \")\n",
    "        if (s + 1) % n_cols == 0: print(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imprima_funcao_valor_de_acao(Q, \n",
    "                                optimal_Q=None, \n",
    "                                action_symbols=('<', '>'), \n",
    "                                prec=3, \n",
    "                                title='Função Q (valor de ação):'):\n",
    "    vf_types=('',) if optimal_Q is None else ('', '*', 'err')\n",
    "    headers = ['s',] + [' '.join(i) for i in list(itertools.product(vf_types, action_symbols))]\n",
    "    print(title)\n",
    "    states = np.arange(len(Q))[..., np.newaxis]\n",
    "    arr = np.hstack((states, np.round(Q, prec)))\n",
    "    if not (optimal_Q is None):\n",
    "        arr = np.hstack((arr, np.round(optimal_Q, prec), np.round(optimal_Q-Q, prec)))\n",
    "    print(tabulate(arr, headers, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imprima_politica(pi, P, action_symbols=('<', 'v', '>', '^'), n_cols=4, title='Policy:'):\n",
    "    print(title)\n",
    "    arrs = {k:v for k,v in enumerate(action_symbols)}\n",
    "    for s in range(len(P)):\n",
    "        a = pi(s)\n",
    "        print(\"| \", end=\"\")\n",
    "        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n",
    "            print(\"\".rjust(9), end=\" \")\n",
    "        else:\n",
    "            print(str(s).zfill(2), arrs[a].rjust(6), end=\" \")\n",
    "        if (s + 1) % n_cols == 0: print(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "def plotar_funcao_de_valor(title, V_track, V_true=None, log=False, limit_value=0.05, limit_items=5):\n",
    "    np.random.seed(123)\n",
    "    per_col = 25\n",
    "    linecycler = cycle([\"-\",\"--\",\":\",\"-.\"])\n",
    "    legends = []\n",
    "\n",
    "    valid_values = np.argwhere(V_track[-1] > limit_value).squeeze()\n",
    "    items_idxs = np.random.choice(valid_values, \n",
    "                                  min(valid_values.size, limit_items), \n",
    "                                  replace=False)\n",
    "    # draw the true values first\n",
    "    if V_true is not None:\n",
    "        for i, state in enumerate(V_track.T):\n",
    "            if i not in items_idxs:\n",
    "                continue\n",
    "            if state[-1] < limit_value:\n",
    "                continue\n",
    "\n",
    "            label = 'v*({})'.format(i)\n",
    "            plt.axhline(y=V_true[i], color='k', linestyle='-', linewidth=1)\n",
    "            plt.text(int(len(V_track)*1.02), V_true[i]+.01, label)\n",
    "\n",
    "    # then the estimates\n",
    "    for i, state in enumerate(V_track.T):\n",
    "        if i not in items_idxs:\n",
    "            continue\n",
    "        if state[-1] < limit_value:\n",
    "            continue\n",
    "        line_type = next(linecycler)\n",
    "        label = 'V({})'.format(i)\n",
    "        p, = plt.plot(state, line_type, label=label, linewidth=3)\n",
    "        legends.append(p)\n",
    "        \n",
    "    legends.reverse()\n",
    "\n",
    "    ls = []\n",
    "    for loc, idx in enumerate(range(0, len(legends), per_col)):\n",
    "        subset = legends[idx:idx+per_col]\n",
    "        l = plt.legend(subset, [p.get_label() for p in subset], \n",
    "                       loc='center right', bbox_to_anchor=(1.25, 0.5))\n",
    "        ls.append(l)\n",
    "    [plt.gca().add_artist(l) for l in ls[:-1]]\n",
    "    if log: plt.xscale('log')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('State-value function')\n",
    "    plt.xlabel('Episodes (log scale)' if log else 'Episodes')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Estratégias de aprendizagem por Reforço**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação do algoritmo de controle de Monte Carlo\n",
    "\n",
    "# Você passa a política o ambiente para qual o algoritmo criará a política, desconto de recompensa, especificações para a criação do array de \n",
    "# decaimento exponencial de alpha e epsilon, número de episódios considerados, quantidade maxima de iterações e se utilizaremos a primeira \n",
    "# vista Monte Carlo ou não\n",
    "def controle_mc(amb, gamma=1.0, ini_alpha=0.5, min_alpha=0.01, taxa_decay_alpha=0.5, ini_epsilon=1.0, min_epsilon=0.1, \n",
    "               taxa_decay_epsilon=0.9, n_episodios=3000, max_iteracoes=200, primeira_visita=True):\n",
    "    # Obtemos o número de estados do ambiente (nS) e o número de ações disponíveis (nA)\n",
    "    nS, nA = amb.observation_space.n, amb.action_space.n\n",
    "\n",
    "    # Calculando todos os descontos possíveis de uma vez. \n",
    "    # Esta função logspace para uma gama de 0,99 e um max_step de 100 retorna um vetor de número 100: [1, 0,99, 0,9801, . . ., 0,3697].\n",
    "    descontos = np.logspace(0, max_iteracoes, num=max_iteracoes, base=gamma, endpoint=False)\n",
    "    # Usando a função criada anteriormente para criar o decaimento exponencial de alpha \n",
    "    alphas = decaimento_exponencial(ini_alpha, min_alpha, taxa_decay_alpha, n_episodios)\n",
    "    # Usando a função criada anteriormente para criar o decaimento exponencial de epsilon \n",
    "    epsilons = decaimento_exponencial(ini_epsilon, min_epsilon, taxa_decay_epsilon, n_episodios)\n",
    "\n",
    "    # Array utilizado para ver o progresso da avaliação da política\n",
    "    pi_historico = []\n",
    "    # Inicializando a função com as dimensões do ambiente analisado e os valores zerados\n",
    "    Q = np.zeros((nS, nA), dtype=np.float64)\n",
    "    # Array utilizado para ver o progresso da função Q a cada episodio\n",
    "    Q_historico = np.zeros((n_episodios, nS, nA), dtype=np.float64)\n",
    "\n",
    "    # Criando a estratégia de seleção de ação: Utilizamos Epsilon Ganancioso\n",
    "    #     Funções anônimas (Lambda): são  funções que o usuário não precisa definir, ou seja, não vai precisar\n",
    "    # escrever a função e depois utilizá-la dentro do código.\n",
    "    seleciona_acao = lambda estado, Q, epsilon: \\\n",
    "        np.argmax(Q[estado]) \\\n",
    "        if np.random.random() > epsilon \\\n",
    "        else np.random.randint(len(Q[estado]))\n",
    "\n",
    "    # Loop principal com tqdm\n",
    "    # e --> Número da iteração\n",
    "    for e in tqdm(range(n_episodios), leave=False):\n",
    "        # Gerando a trajetória com a função criada anteriormente\n",
    "        trajetoria = gerar_trajetoria_epsilon_ganancioso(seleciona_acao, Q, epsilons[e], amb, max_iteracoes)\n",
    "\n",
    "        # Criamos o array que verifica se a trajetória passou pelo mesmo estado\n",
    "        visitado = np.zeros((nS,nA), dtype=np.bool)\n",
    "\n",
    "        # Loop secundário\n",
    "        # t --> Número da iteração\n",
    "        # (estado, acao, recompensa, _, _) --> Tupla de experiencia da trajetória\n",
    "        for t, (estado, acao, recompensa, _, _) in enumerate(trajetoria):\n",
    "            # Caso o estado já foi visitado e se estivermos usando a primeira visita MC\n",
    "            if visitado[estado][acao] and primeira_visita:\n",
    "                # Ignoramos tudo e vamos para a próxima iteração do loop\n",
    "                continue\n",
    "\n",
    "            # Marcamos que estado foi visitado\n",
    "            visitado[estado][acao] = True\n",
    "\n",
    "            # Descobrindo a quantidade de passos até o estado terminal\n",
    "            n_passos = len(trajetoria[t:])\n",
    "            # Calculando o Retorno: Somando todas recompensas (com seus respectivos descontos)\n",
    "            G = np.sum(descontos[:n_passos] * trajetoria[t:, 2])\n",
    "            # Atualizamos a Função Q (Aplicação da equação de Monte Carlo)\n",
    "            Q[estado][acao] = Q[estado][acao] + alphas[e] * (G - Q[estado][acao])\n",
    "\n",
    "        # Guardamos a Função Q atual no histórico\n",
    "        Q_historico[e] = Q\n",
    "        # Guardamos a política atual no histórico\n",
    "        pi_historico.append(np.argmax(Q, axis=1))\n",
    "\n",
    "    # Extraímos a Função de Valor de Estado selecionando as melhores ações de Q\n",
    "    V = np.max(Q, axis=1)\n",
    "    # Com a Função de Valor Feita podemos obter uma política ótima\n",
    "    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "    return Q, V, pi, Q_historico, pi_historico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação do algoritmo SARSA\n",
    "\n",
    "# Você passa a política o ambiente para qual o algoritmo criará a política, desconto de recompensa, especificações para a criação do array de \n",
    "# decaimento exponencial de alpha e epsilon, número de episódios considerados\n",
    "def sarsa(amb, gamma=1.0, ini_alpha=0.5, min_alpha=0.01, taxa_decay_alpha=0.5, ini_epsilon=1.0, \n",
    "          min_epsilon=0.1, taxa_decay_epsilon=0.9, n_episodios=3000):\n",
    "    # Obtemos o número de estados do ambiente (nS) e o número de ações disponíveis (nA)\n",
    "    nS, nA = env.observation_space.n, env.action_space.n\n",
    "\n",
    "    # Array utilizado para ver o progresso da avaliação da política\n",
    "    pi_historico = []\n",
    "    # Inicializando a função com as dimensões do ambiente analisado e os valores zerados\n",
    "    Q = np.zeros((nS, nA), dtype=np.float64)\n",
    "    # Array utilizado para ver o progresso da função Q a cada episodio\n",
    "    Q_historico = np.zeros((n_episodios, nS, nA), dtype=np.float64)\n",
    "\n",
    "    # Criando a estratégia de seleção de ação: Utilizamos Epsilon Ganancioso\n",
    "    #     Funções anônimas (Lambda): são  funções que o usuário não precisa definir, ou seja, não vai precisar\n",
    "    # escrever a função e depois utilizá-la dentro do código.\n",
    "    seleciona_acao = lambda estado, Q, epsilon: \\\n",
    "        np.argmax(Q[estado]) \\\n",
    "        if np.random.random() > epsilon \\\n",
    "        else np.random.randint(len(Q[estado]))\n",
    "\n",
    "    # Usando a função criada anteriormente para criar o decaimento exponencial de alpha \n",
    "    alphas = decaimento_exponencial(ini_alpha, min_alpha, taxa_decay_alpha, n_episodios)\n",
    "    # Usando a função criada anteriormente para criar o decaimento exponencial de epsilon \n",
    "    epsilons = decaimento_exponencial(ini_epsilon, min_epsilon, taxa_decay_epsilon, n_episodios)\n",
    "\n",
    "    # Loop principal com tqdm\n",
    "    # e --> Número da iteração\n",
    "    for e in tqdm(range(n_episodios), leave=False):\n",
    "        # Resetamos o ambiente para garantir que ele esteja pronto para ser analisado\n",
    "        # Obtemos o estado inicial e se o estado é terminal\n",
    "        estado, terminado = amb.reset(), False\n",
    "        # Selecionamos a acao para o estado atual de acordo com a estratégia passada\n",
    "        acao = seleciona_acao(estado, Q, epsilons[e])\n",
    "        \n",
    "        # Loop secundário --> Só para se entrarmos em um estado terminal\n",
    "        while not terminado:\n",
    "            # Obtemos as informações de retorno do passo dado\n",
    "            prox_estado, recompensa, terminado, _ = amb.step(acao)\n",
    "\n",
    "            # Selecionamos a acao para o estado atual de acordo com a estratégia passada\n",
    "            # Observe que antes de fazer qualquer cálculo, precisamos obter a ação para a próxima etapa.\n",
    "            prox_acao = seleciona_acao(prox_estado, Q, epsilons[e])\n",
    "\n",
    "            # Implementação da função TD: Calculando o objetivo (Caso o estado seja terminal temos que zerar)\n",
    "            objetivo_td = recompensa + gamma * Q[prox_estado][prox_acao] * (not terminado)\n",
    "            # Implementação da função TD: Calculando o erro\n",
    "            erro_td = objetivo_td - Q[estado][acao]\n",
    "            # Implementação da função TD: Calculando a função Q\n",
    "            Q[estado][acao] = Q[estado][acao] + alphas[e] * erro_td\n",
    "\n",
    "            # Passamos para o proximo estado e proxima ação\n",
    "            estado, acao = prox_estado, prox_acao\n",
    "\n",
    "        # Guardamos a Função Q atual no histórico\n",
    "        Q_historico[e] = Q\n",
    "        # Guardamos a política atual no histórico\n",
    "        pi_historico.append(np.argmax(Q, axis=1))\n",
    "\n",
    "    # Extraímos a Função de Valor de Estado selecionando as melhores ações de Q\n",
    "    V = np.max(Q, axis=1)\n",
    "    # Com a Função de Valor Feita podemos obter uma política ótima\n",
    "    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "    return Q, V, pi, Q_historico, pi_historico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação do algoritmo Q-Learning\n",
    "\n",
    "from gymnasium.spaces import Discrete, MultiDiscrete\n",
    "from gymnasium.spaces.utils import flatdim\n",
    "\n",
    "# Você passa a política o ambiente para qual o algoritmo criará a política, desconto de recompensa, especificações para a criação do array de \n",
    "# decaimento exponencial de alpha e epsilon, número de episódios considerados\n",
    "def q_learning(amb, gamma=1.0, ini_alpha=0.5, min_alpha=0.01, taxa_decay_alpha=0.5, ini_epsilon=1.0, \n",
    "          min_epsilon=0.1, taxa_decay_epsilon=0.9, n_episodios=3000):\n",
    "    # Obtemos o número de estados do ambiente (nS) e o número de ações disponíveis (nA)\n",
    "    nS = flatdim(amb.observation_space) if isinstance(amb.observation_space, MultiDiscrete) else amb.observation_space.n\n",
    "    # nA = flatdim(amb.action_space) if isinstance(amb.action_space, MultiDiscrete) else amb.action_space.n\n",
    "    nA = flatdim(amb.action_space) if isinstance(amb.action_space, Discrete) else amb.action_space.n\n",
    "    print(f\"{nS=} {nA=}\")\n",
    "\n",
    "    # Array utilizado para ver o progresso da avaliação da política\n",
    "    pi_historico = []\n",
    "    # Inicializando a função com as dimensões do ambiente analisado e os valores zerados\n",
    "    Q = np.zeros((nS, nA), dtype=np.float64)\n",
    "    print(f'{Q.shape=}')\n",
    "    # Array utilizado para ver o progresso da função Q a cada episodio\n",
    "    Q_historico = np.zeros((n_episodios, nS, nA), dtype=np.float64)\n",
    "\n",
    "    # Criando a estratégia de seleção de ação: Utilizamos Epsilon Ganancioso\n",
    "    #     Funções anônimas (Lambda): são  funções que o usuário não precisa definir, ou seja, não vai precisar\n",
    "    # escrever a função e depois utilizá-la dentro do código.\n",
    "    seleciona_acao = lambda estado, Q, epsilon: \\\n",
    "        np.argmax(Q[estado]) \\\n",
    "        if np.random.random() > epsilon \\\n",
    "        else np.random.randint(len(Q[estado]))\n",
    "\n",
    "    # Usando a função criada anteriormente para criar o decaimento exponencial de alpha \n",
    "    alphas = decaimento_exponencial(ini_alpha, min_alpha, taxa_decay_alpha, n_episodios)\n",
    "    # Usando a função criada anteriormente para criar o decaimento exponencial de epsilon \n",
    "    epsilons = decaimento_exponencial(ini_epsilon, min_epsilon, taxa_decay_epsilon, n_episodios)\n",
    "\n",
    "    # Loop principal com tqdm\n",
    "    # e --> Número da iteração\n",
    "    for e in tqdm(range(n_episodios), leave=False):\n",
    "        print(f'{e=}')\n",
    "        # Resetamos o ambiente para garantir que ele esteja pronto para ser analisado\n",
    "        # Obtemos o estado inicial e se o estado é terminal\n",
    "        estado, terminado = amb.reset(), False\n",
    "        print(f'{terminado=}')\n",
    "        # Loop secundário --> Só para se entrarmos em um estado terminal\n",
    "        while not terminado:            \n",
    "            # Selecionamos a acao para o estado atual de acordo com a estratégia passada\n",
    "            acao = seleciona_acao(estado, Q, epsilons[e])\n",
    "            print(f'ANTES: {estado=} {acao=} {Q[estado][acao]=}')\n",
    "            # Obtemos as informações de retorno do passo dado\n",
    "            prox_estado, recompensa, terminado, _ = amb.step(acao)\n",
    "            print(f'{acao=} {prox_estado=} {recompensa=} {terminado=}')\n",
    "            # Implementação da função Q-learning: Calculando o objetivo (Caso o estado seja terminal temos que zerar)\n",
    "            objetivo_td = recompensa + gamma * Q[prox_estado].max() * (not terminado)\n",
    "            # Implementação da função Q-learning: Calculando o erro\n",
    "            erro_td = objetivo_td - Q[estado][acao]            \n",
    "            # Implementação da função Q-learning: Calculando a Função Q\n",
    "            Q[estado][acao] = Q[estado][acao] + alphas[e] * erro_td\n",
    "            print(f'{objetivo_td=} {erro_td=} {Q[estado][acao]=}')\n",
    "\n",
    "            # Passamos para o proximo estado\n",
    "            estado = prox_estado      \n",
    "\n",
    "        # Guardamos a Função Q atual no histórico\n",
    "        Q_historico[e] = Q\n",
    "        # Guardamos a política atual no histórico\n",
    "        pi_historico.append(np.argmax(Q, axis=1))\n",
    "\n",
    "    # Extraímos a Função de Valor de Estado selecionando as melhores ações de Q\n",
    "    V = np.max(Q, axis=1)\n",
    "    # Com a Função de Valor Feita podemos obter uma política ótima\n",
    "    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "    return Q, V, pi, Q_historico, pi_historico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação do algoritmo Q-Learning duplo\n",
    "\n",
    "# Você passa a política o ambiente para qual o algoritmo criará a política, desconto de recompensa, especificações para a criação do array de \n",
    "# decaimento exponencial de alpha e epsilon, número de episódios considerados\n",
    "def double_q_learning(amb, gamma=1.0, ini_alpha=0.5, min_alpha=0.01, taxa_decay_alpha=0.5, ini_epsilon=1.0,\n",
    "                      min_epsilon=0.1, taxa_decay_epsilon=0.9, n_episodios=3000):\n",
    "    # Obtemos o número de estados do ambiente (nS) e o número de ações disponíveis (nA)\n",
    "    nS, nA = amb.observation_space.n, amb.action_space.n\n",
    "    \n",
    "    # Array utilizado para ver o progresso da avaliação da política\n",
    "    pi_historico = []\n",
    "\n",
    "     # Inicializando a função Q1 e Q2 com as dimensões do ambiente analisado e os valores zerados\n",
    "    Q1 = np.zeros((nS, nA), dtype=np.float64)\n",
    "    Q2 = np.zeros((nS, nA), dtype=np.float64)\n",
    "    # Arrays utilizados para ver o progresso da funções Q1 e Q2 a cada episodio\n",
    "    Q1_historico = np.zeros((n_episodios, nS, nA), dtype=np.float64)\n",
    "    Q2_historico = np.zeros((n_episodios, nS, nA), dtype=np.float64)\n",
    "\n",
    "    # Criando a estratégia de seleção de ação: Utilizamos Epsilon Ganancioso\n",
    "    #     Funções anônimas (Lambda): são  funções que o usuário não precisa definir, ou seja, não vai precisar\n",
    "    # escrever a função e depois utilizá-la dentro do código.\n",
    "    seleciona_acao = lambda estado, Q, epsilon: \\\n",
    "        np.argmax(Q[estado]) \\\n",
    "        if np.random.random() > epsilon \\\n",
    "        else np.random.randint(len(Q[estado]))\n",
    "\n",
    "    # Usando a função criada anteriormente para criar o decaimento exponencial de alpha \n",
    "    alphas = decaimento_exponencial(ini_alpha, min_alpha, taxa_decay_alpha, n_episodios)\n",
    "    # Usando a função criada anteriormente para criar o decaimento exponencial de epsilon \n",
    "    epsilons = decaimento_exponencial(ini_epsilon, min_epsilon, taxa_decay_epsilon, n_episodios)\n",
    "\n",
    "    # Loop principal com tqdm\n",
    "    # e --> Número da iteração\n",
    "    for e in tqdm(range(n_episodios), leave=False):\n",
    "        # Resetamos o ambiente para garantir que ele esteja pronto para ser analisado\n",
    "        # Obtemos o estado inicial e se o estado é terminal\n",
    "        estado, terminado = amb.reset(), False\n",
    "\n",
    "        # Loop secundário --> Só para se entrarmos em um estado terminal\n",
    "        while not terminado:\n",
    "            # Selecionamos a acao para o estado atual de acordo com a estratégia passada\n",
    "            # Precisamos passar somente uma função Q por isso criamos um array com a média de nossas duas funções Q\n",
    "            # Também poderíamos usar a soma de nossas funções Q aqui, eles darão resultados semelhantes.\n",
    "            acao = seleciona_acao(estado, (Q1 + Q2)/2., epsilons[e])\n",
    "\n",
    "            # Obtemos as informações de retorno do passo dado\n",
    "            prox_estado, recompensa, terminado, _ = amb.step(acao)\n",
    "\n",
    "            # Escolhemos aleatoriamente (50%) qual função Q usar\n",
    "            if np.random.randint(2):\n",
    "                # Vamos usar a ação que Q1 acha melhor\n",
    "                argmax_Q1 = np.argmax(Q1[prox_estado])\n",
    "\n",
    "                # Mas usamos o valor de Q2 para calcular o objetivo TD\n",
    "                objetivo_td = recompensa + gamma * Q2[prox_estado][argmax_Q1] * (not terminado)\n",
    "                # Resto da implementação da função do Q-learning: Usando Q1\n",
    "                erro_td = objetivo_td - Q1[estado][acao]\n",
    "                Q1[estado][acao] = Q1[estado][acao] + alphas[e] * erro_td\n",
    "\n",
    "            else:\n",
    "                # Vamos usar a ação que Q2 acha melhor\n",
    "                argmax_Q2 = np.argmax(Q2[prox_estado])\n",
    "\n",
    "                # Mas usamos o valor de Q1 para calcular o objetivo TD\n",
    "                objetivo_td = recompensa + gamma * Q1[prox_estado][argmax_Q2] * (not terminado)\n",
    "                # Resto da implementação da função do Q-learning: Usando Q2\n",
    "                erro_td = objetivo_td - Q2[estado][acao]\n",
    "                Q2[estado][acao] = Q2[estado][acao] + alphas[e] * erro_td\n",
    "\n",
    "            # Passamos para o proximo estado\n",
    "            estado = prox_estado\n",
    "\n",
    "        # Guardamos as Funções Q1 e Q2 atuais no histórico\n",
    "        Q1_historico[e] = Q1\n",
    "        Q2_historico[e] = Q2\n",
    "        # Guardamos a política atual no histórico, perceba que utilizaremos a avaliação média entre Q1 e Q2\n",
    "        pi_historico.append(np.argmax((Q1 + Q2)/2., axis=1))\n",
    "\n",
    "    # O Q final é média do 1 com o 2\n",
    "    Q = (Q1 + Q2)/2.\n",
    "    # Obtemos a Função valor estado\n",
    "    V = np.max(Q, axis=1)\n",
    "    # Obtemos uma política ótima\n",
    "    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "    # Perceba que retornamos o histórico médio de Q\n",
    "    return Q, V, pi, (Q1_historico + Q2_historico)/2., pi_historico"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementação de Ambientes customizados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from typing import Any, Optional\n",
    "from collections import deque\n",
    "\n",
    "class BeerGame(gym.Env):\n",
    "\n",
    "    # Método 'construtor'\n",
    "    def __init__(self, seed : Optional[int] = None) -> None: \n",
    "       # 1-> Definindo o espaço de ações \n",
    "       self.pedido_max = 20 # Podem ser pedidos somente 20 cervejas por vez\n",
    "       self.pedido_min = 0 # Não é possível realizar pedidos negativos (Isso não faz sentido!!)\n",
    "       self.pedido_ini = 4 # O pedido inicial sempre é 4\n",
    "       self.pedido_ant = self.pedido_ini # Essa variável 'pedido_ant' é uma variável auxiliar que guarda a informação sobre o pedido feito anteriormente (será atualizada a cada iteração)\n",
    "       # Declarando espaço de ações:\n",
    "       self.action_space = spaces.MultiDiscrete(np.array(4*[self.pedido_max]))\n",
    "       \n",
    "       # 2-> Definindo uma seed\n",
    "       self.seed(seed) # Chamamos o método que cria uma seed\n",
    "       \n",
    "       # 3-> Definindo o espaço de estados\n",
    "       self.estoque_ini = 12 # A quantidade inicial em estoque sempre é 12\n",
    "       self.transporte_ini = 4 # A quantidade inicial em transporte sempre é 4\n",
    "       self.capacidade_max = 500 # A capacidade máxima de estoque em qualquer um dos membros da cadeia de suprimentos é 500 \n",
    "       self.capacidade_min = 0 # Não é possível realizar guardar uma quantidade de cerveja negativa (Isso não faz sentido!!)\n",
    "       # Definindo o observation space: limites, tipo de espaço e forma\n",
    "       self.observation_space = spaces.MultiDiscrete(np.array(12*[self.capacidade_max]+[self.pedido_max]))\n",
    "\n",
    "       # 4-> Inicializando os arrays dos estoques, transporte e produção\n",
    "       self.estoques = 4*[0] # Simples 4 vetores de 1 posição uma para cada (R, W, D, F)\n",
    "       self.entrega_retailer = deque(2*[0]) # Fila com 2 posições Retailer - Transporte\n",
    "       self.entrega_wholesailer = deque(2*[0]) # Fila com 2 posições Wholesaler - Transporte\n",
    "       self.entrega_distributor = deque(2*[0]) # Fila com 2 posições Distributor - Transporte\n",
    "       self.producao_factory = deque(2*[0]) # Fila com 2 posições Factory - Produção\n",
    "       \n",
    "       # 5-> Definimos os arrays com os valores iniciais correspondentes\n",
    "       self._inicializar_cadeia()\n",
    "       \n",
    "       # 6-> Definimos os preços \n",
    "       self.custo_de_estoque = 0.5\n",
    "       self.custo_de_backlog = 1\n",
    "       \n",
    "       # 6-> Definimos o oeríodo em que o BeerGame será rodado\n",
    "       self.periodo_max = 50\n",
    "       self.periodo_atual = 0 # Variável auxiliar será atualizada a cada iteração\n",
    "\n",
    "    # Define os arrays com os valores iniciais correspondentes\n",
    "    def _inicializar_cadeia(self) -> None:\n",
    "       self.estoques = 4*[self.estoque_ini] # Estoques começam com 12 cervejas\n",
    "       self.entrega_retailer = deque(2*[self.transporte_ini]) # Estão transportando 4 cervejas em cada slot de entrega\n",
    "       self.entrega_wholesaler = deque(2*[self.transporte_ini]) # Estão transportando 4 cervejas em cada slot de entrega\n",
    "       self.entrega_distributor = deque(2*[self.transporte_ini]) # Estão transportando 4 cervejas em cada slot de entrega\n",
    "       self.producao_factory = deque(2*[self.transporte_ini]) # Estão produzindo 4 cervejas em cada slot de produção\n",
    "\n",
    "    #   Esse método retorna um estado, ou seja, uma observação do estado, em que a \n",
    "    # posição 0 é estoque do Retailer e a posicao 11 é slot de producao da fábrica\n",
    "    # a posição 12 é na verdade a informação sobre o pedido feito ao Retailer anteriormente\n",
    "    def _montar_estado(self) -> list[int]:\n",
    "       estado = 13*[0]\n",
    "       estado[0] = self.estoques[0]\n",
    "       estado[1] = self.entrega_retailer[0]\n",
    "       estado[2] = self.entrega_retailer[1]\n",
    "       estado[3] = self.estoques[1]\n",
    "       estado[4] = self.entrega_wholesaler[0]\n",
    "       estado[5] = self.entrega_wholesaler[1]\n",
    "       estado[6] = self.estoques[2]\n",
    "       estado[7] = self.entrega_distributor[0]\n",
    "       estado[8] = self.entrega_distributor[1]\n",
    "       estado[9] = self.estoques[3]\n",
    "       estado[10] = self.producao_factory[0]\n",
    "       estado[11] = self.producao_factory[1]\n",
    "       estado[12] = self.pedido_ant\n",
    "       return estado\n",
    "\n",
    "    # Esse método reseta o ambiente, reinicializando os arrays com os valores iniciais definidos no 'construtor'\n",
    "    def reset(self) -> list[int]:\n",
    "       self._inicializar_cadeia() # Reinicializa os arrays com os valores iniciais\n",
    "       self.periodo_atual = 0 # Reinicia a contagem do período\n",
    "       return np.array(self._montar_estado()) # retorna uma observação do estado inicial\n",
    "    \n",
    "    # Esse método executa a ação escolhida no ambiente e retorna uma observação do estado, recompensa, se o estado é terminal e uma informação\n",
    "    def step(self, action : list[int]) -> tuple[list[int], float, bool, bool, dict]:\n",
    "       self.periodo_atual += 1 # Cada vez que uma ação é tomada é considerado um novo período\n",
    "       \n",
    "       recompensa = 0 # Inicializamos o valor da recompensa\n",
    "       self.pedido_ant = self.rand_generator.randint(self.pedido_max + 1) # Sorteamos um número (de 1 a 20) que será o número de cervejas solicitados pelo cliente ao Retailer\n",
    "       \n",
    "       # Chegaram os caminhões: Os pedidos feitos anteriormente que estavam aguardando nas filas de entrega é adicionado aos estoques\n",
    "       self.estoques[0] += self.entrega_retailer.popleft()\n",
    "       self.estoques[1] += self.entrega_wholesaler.popleft()\n",
    "       self.estoques[2] += self.entrega_distributor.popleft()\n",
    "       self.estoques[3] += self.producao_factory.popleft()\n",
    "       \n",
    "       # Decisão Retailer:\n",
    "       if (self.estoques[0] >= self.pedido_ant):\n",
    "         # O número de unidades estocadas é subtraído pelo pedido quando o estoque é maior ou igual ao pedido\n",
    "         self.estoques[0] = self.estoques[0] - self.pedido_ant\n",
    "       else:\n",
    "         # Quando pedido é maior cobra-se um custo de backlog a cada unidade de cerveja não atendida, esse custo é armazenado como recompensa\n",
    "         recompensa += (self.pedido_ant - self.estoques[0])*self.custo_de_backlog \n",
    "         self.estoques[0] = 0 # E o estoque é zerado\n",
    "       \n",
    "       # Decisão Wholesaler:\n",
    "       if (self.estoques[1] >= action[0]):\n",
    "         # O número de unidades estocadas é subtraído pelo pedido quando o estoque é maior ou igual ao pedido\n",
    "         self.estoques[1] = self.estoques[1] - action[0]\n",
    "         # Como Wholesaler não lida diretamente com cliente ele manda transportar as unidades de cerveja: o número pedido é adicionado a fila\n",
    "         self.entrega_retailer.append(action[0])\n",
    "       else:\n",
    "         # Quando pedido é maior cobra-se um custo de backlog a cada unidade de cerveja não atendida, esse custo é armazenado como recompensa\n",
    "         recompensa += (action[0] - self.estoques[1])*self.custo_de_backlog\n",
    "         # A quantidade que tem estoque será então transportada: adiciona-se a fila\n",
    "         self.entrega_retailer.append(self.estoques[1])\n",
    "         self.estoques[1] = 0 # E o estoque é zerado\n",
    "\n",
    "       # Decisão Distributor:\n",
    "       if (self.estoques[2] >= action[1]):\n",
    "         # O número de unidades estocadas é subtraído pelo pedido quando o estoque é maior ou igual ao pedido\n",
    "         self.estoques[2] = self.estoques[2] - action[1]\n",
    "         # Como Distributor não lida diretamente com cliente ele manda transportar as unidades de cerveja: o número pedido é adicionado a fila\n",
    "         self.entrega_wholesaler.append(action[1])\n",
    "       else:\n",
    "         # Quando pedido é maior cobra-se um custo de backlog a cada unidade de cerveja não atendida, esse custo é armazenado como recompensa\n",
    "         recompensa += (action[1] - self.estoques[2])*self.custo_de_backlog\n",
    "         # A quantidade que tem estoque será então transportada: adiciona-se a fila\n",
    "         self.entrega_wholesaler.append(self.estoques[2])\n",
    "         self.estoques[2] = 0 # E o estoque é zerado\n",
    "       \n",
    "       # Decisão Factory:\n",
    "       if (self.estoques[3] >= action[2]):\n",
    "         # O número de unidades estocadas é subtraído pelo pedido quando o estoque é maior ou igual ao pedido\n",
    "         self.estoques[3] = self.estoques[3] - action[2]\n",
    "         # Como Factory não lida diretamente com cliente ele manda transportar as unidades de cerveja: o número pedido é adicionado a fila\n",
    "         self.entrega_distributor.append(action[2])\n",
    "       else:\n",
    "         # Quando pedido é maior cobra-se um custo de backlog a cada unidade de cerveja não atendida, esse custo é armazenado como recompensa\n",
    "         recompensa += (action[2] - self.estoques[3])*self.custo_de_backlog\n",
    "         # A quantidade que tem estoque será então transportada: adiciona-se a fila\n",
    "         self.entrega_distributor.append(self.estoques[3])\n",
    "         self.estoques[3] = 0 # E o estoque é zerado\n",
    "       \n",
    "       # Da início a produção do número de unidades pedidas a fábrica\n",
    "       self.producao_factory.append(action[3])\n",
    "       \n",
    "       # Além dos custos de Backlog é adicionado o custo de estoque por cada unidade em estoque em (R, W, D, F) -> Perceba que nessa situação a recompensa é negativa\n",
    "       recompensa += sum(self.estoques)*self.custo_de_estoque\n",
    "       # BeerGame só termina quando é atinjido o 50º período\n",
    "       terminado = self.periodo_atual == self.periodo_max\n",
    "\n",
    "       return np.array(self._montar_estado()), recompensa, terminado, {} # Retorna-se a observação do estado, a recompensa, se é estado terminal  \n",
    "\n",
    "    # Esse método imprime uma observação do estado\n",
    "    def render(self, mode : str =\"human\") -> Any: # opcional\n",
    "       print(self._montar_estado())\n",
    "\n",
    "    # Esse método close fecha todos os recursos abertos que foram usados pelo ambiente. No nosso caso não foi preciso implementá-lo\n",
    "    def close(self) -> None: # opcional\n",
    "       pass\n",
    "    \n",
    "    #   Cria uma seed aleatória para iteração: afeta diretamente os pedidos aleatórios do suposto cliente\n",
    "    # Dessa forma a seed é utilizada quando deseja repetir um certo cenário no ambiente e testá-lo de diferentes formas\n",
    "    def seed(self, seed=None) -> None:\n",
    "      self.rand_generator = np.random.RandomState(seed)\n",
    "      self.action_space.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeerGameSimplificado(BeerGame):\n",
    "  \n",
    "  def __init__(self,  seed : Optional[int] = None) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # Reescrevendo a variável 'action_space' para aceitar somente 1 ação:\n",
    "        self.action_space = spaces.Discrete(20)\n",
    "\n",
    "  def step(self, action : int) -> tuple[list[int], float, bool, bool, dict]:\n",
    "        # Recebe uma única ação e cria uma lista com a ação replicada para então chamar o método da Classe Pai\n",
    "        acao = [action, action, action, action]\n",
    "        \n",
    "        return super().step(acao)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Radiação.jpg](data:image/jpeg;base64,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)\n",
    "## **Área de Testes**\n",
    "\n",
    "### **Como debugar no Google Colab?**\n",
    "* https://stackoverflow.com/questions/52656692/debugging-in-google-colabo-google-colab-94582\n",
    "\n",
    "Command  Description\n",
    "\n",
    "* list     Show the current location in the file\n",
    "* h(elp)   Show a list of commands, or find help on a specific command\n",
    "* q(uit)   Quit the debugger and the program\n",
    "* c(ontinue)  Quit the debugger, continue in the program\n",
    "* n(ext)   Go to the next step of the program\n",
    "* <enter>  Repeat the previous command\n",
    "* p(rint)  Print variables\n",
    "* s(tep)   Step into a subroutine\n",
    "* r(eturn)    Return out of a subroutine\n",
    "\n",
    "### **Pixie_debugger -> Problema com versão do jinja2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_10672\\2698045853.py:17: RuntimeWarning: invalid value encountered in true_divide\n",
      "  valor = (valor - valor.min()) / (valor.max() - valor.min())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estado inicial [12  4  4 12  4  4 12  4  4 12  4  4  4]\n",
      "nS=6020 nA=20\n",
      "Q.shape=(6020, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=0\n",
      "terminado=False\n",
      "ANTES: estado=array([12,  4,  4, 12,  4,  4, 12,  4,  4, 12,  4,  4,  4]) acao=10 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=10 prox_estado=array([15,  4, 10,  6,  4, 10,  6,  4, 10,  6,  4, 10,  1]) recompensa=16.5 terminado=False\n",
      "objetivo_td=16.5 erro_td=array([16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5,\n",
      "       16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([15,  4, 10,  6,  4, 10,  6,  4, 10,  6,  4, 10,  1]) acao=12 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=12 prox_estado=array([15, 10, 10,  0, 10, 10,  0, 10, 10,  0, 10, 12,  4]) recompensa=13.5 terminado=False\n",
      "objetivo_td=13.5 erro_td=array([13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5,\n",
      "       13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([15, 10, 10,  0, 10, 10,  0, 10, 10,  0, 10, 12,  4]) acao=0 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=0 prox_estado=array([18, 10,  0, 10, 10,  0, 10, 10,  0, 10, 12,  0,  7]) recompensa=24.0 terminado=False\n",
      "objetivo_td=24.0 erro_td=array([24., 24., 24., 24., 24., 24., 24., 24., 24., 24., 24., 24., 24.,\n",
      "       24., 24., 24., 24., 24., 24., 24.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([18, 10,  0, 10, 10,  0, 10, 10,  0, 10, 12,  0,  7]) acao=2 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=2 prox_estado=array([26,  0,  2, 18,  0,  2, 18,  0,  2, 20,  0,  2,  2]) recompensa=41.0 terminado=False\n",
      "objetivo_td=41.0 erro_td=array([41., 41., 41., 41., 41., 41., 41., 41., 41., 41., 41., 41., 41.,\n",
      "       41., 41., 41., 41., 41., 41., 41.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([26,  0,  2, 18,  0,  2, 18,  0,  2, 20,  0,  2,  2]) acao=11 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=11 prox_estado=array([22,  2, 11,  7,  2, 11,  7,  2, 11,  9,  2, 11,  4]) recompensa=22.5 terminado=False\n",
      "objetivo_td=22.5 erro_td=array([22.5, 22.5, 22.5, 22.5, 22.5, 22.5, 22.5, 22.5, 22.5, 22.5, 22.5,\n",
      "       22.5, 22.5, 22.5, 22.5, 22.5, 22.5, 22.5, 22.5, 22.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([22,  2, 11,  7,  2, 11,  7,  2, 11,  9,  2, 11,  4]) acao=11 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=11 prox_estado=array([17, 11,  9,  0, 11,  9,  0, 11, 11,  0, 11, 11,  7]) recompensa=12.5 terminado=False\n",
      "objetivo_td=12.5 erro_td=array([12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5,\n",
      "       12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([17, 11,  9,  0, 11,  9,  0, 11, 11,  0, 11, 11,  7]) acao=8 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=8 prox_estado=array([11,  9,  8,  3,  9,  8,  3, 11,  8,  3, 11,  8, 17]) recompensa=10.0 terminado=False\n",
      "objetivo_td=10.0 erro_td=array([10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
      "       10., 10., 10., 10., 10., 10., 10.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([11,  9,  8,  3,  9,  8,  3, 11,  8,  3, 11,  8, 17]) acao=11 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=11 prox_estado=array([15,  8, 11,  1,  8, 11,  3,  8, 11,  3,  8, 11,  5]) recompensa=11.0 terminado=False\n",
      "objetivo_td=11.0 erro_td=array([11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
      "       11., 11., 11., 11., 11., 11., 11.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([15,  8, 11,  1,  8, 11,  3,  8, 11,  3,  8, 11,  5]) acao=8 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=8 prox_estado=array([16, 11,  8,  1, 11,  8,  3, 11,  8,  3, 11,  8,  7]) recompensa=11.5 terminado=False\n",
      "objetivo_td=11.5 erro_td=array([11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5,\n",
      "       11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([16, 11,  8,  1, 11,  8,  3, 11,  8,  3, 11,  8,  7]) acao=6 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=6 prox_estado=array([21,  8,  6,  6,  8,  6,  8,  8,  6,  8,  8,  6,  6]) recompensa=21.5 terminado=False\n",
      "objetivo_td=21.5 erro_td=array([21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5,\n",
      "       21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([21,  8,  6,  6,  8,  6,  8,  8,  6,  8,  8,  6,  6]) acao=1 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=1 prox_estado=array([25,  6,  1, 13,  6,  1, 15,  6,  1, 15,  6,  1,  4]) recompensa=34.0 terminado=False\n",
      "objetivo_td=34.0 erro_td=array([34., 34., 34., 34., 34., 34., 34., 34., 34., 34., 34., 34., 34.,\n",
      "       34., 34., 34., 34., 34., 34., 34.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([25,  6,  1, 13,  6,  1, 15,  6,  1, 15,  6,  1,  4]) acao=6 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=6 prox_estado=array([22,  1,  6, 13,  1,  6, 15,  1,  6, 15,  1,  6,  9]) recompensa=32.5 terminado=False\n",
      "objetivo_td=32.5 erro_td=array([32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5,\n",
      "       32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([22,  1,  6, 13,  1,  6, 15,  1,  6, 15,  1,  6,  9]) acao=11 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=11 prox_estado=array([14,  6, 11,  3,  6, 11,  5,  6, 11,  5,  6, 11,  9]) recompensa=13.5 terminado=False\n",
      "objetivo_td=13.5 erro_td=array([13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5,\n",
      "       13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([14,  6, 11,  3,  6, 11,  5,  6, 11,  5,  6, 11,  9]) acao=2 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=2 prox_estado=array([11, 11,  2,  7, 11,  2,  9, 11,  2,  9, 11,  2,  9]) recompensa=18.0 terminado=False\n",
      "objetivo_td=18.0 erro_td=array([18., 18., 18., 18., 18., 18., 18., 18., 18., 18., 18., 18., 18.,\n",
      "       18., 18., 18., 18., 18., 18., 18.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([11, 11,  2,  7, 11,  2,  9, 11,  2,  9, 11,  2,  9]) acao=10 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=10 prox_estado=array([20,  2, 10,  8,  2, 10, 10,  2, 10, 10,  2, 10,  2]) recompensa=24.0 terminado=False\n",
      "objetivo_td=24.0 erro_td=array([24., 24., 24., 24., 24., 24., 24., 24., 24., 24., 24., 24., 24.,\n",
      "       24., 24., 24., 24., 24., 24., 24.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([20,  2, 10,  8,  2, 10, 10,  2, 10, 10,  2, 10,  2]) acao=10 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=10 prox_estado=array([12, 10, 10,  0, 10, 10,  2, 10, 10,  2, 10, 10, 10]) recompensa=8.0 terminado=False\n",
      "objetivo_td=8.0 erro_td=array([8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8.,\n",
      "       8., 8., 8.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([12, 10, 10,  0, 10, 10,  2, 10, 10,  2, 10, 10, 10]) acao=6 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=6 prox_estado=array([17, 10,  6,  4, 10,  6,  6, 10,  6,  6, 10,  6,  5]) recompensa=16.5 terminado=False\n",
      "objetivo_td=16.5 erro_td=array([16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5,\n",
      "       16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([17, 10,  6,  4, 10,  6,  6, 10,  6,  6, 10,  6,  5]) acao=3 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=3 prox_estado=array([26,  6,  3, 11,  6,  3, 13,  6,  3, 13,  6,  3,  1]) recompensa=31.5 terminado=False\n",
      "objetivo_td=31.5 erro_td=array([31.5, 31.5, 31.5, 31.5, 31.5, 31.5, 31.5, 31.5, 31.5, 31.5, 31.5,\n",
      "       31.5, 31.5, 31.5, 31.5, 31.5, 31.5, 31.5, 31.5, 31.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([26,  6,  3, 11,  6,  3, 13,  6,  3, 13,  6,  3,  1]) acao=3 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=3 prox_estado=array([30,  3,  3, 14,  3,  3, 16,  3,  3, 16,  3,  3,  2]) recompensa=38.0 terminado=False\n",
      "objetivo_td=38.0 erro_td=array([38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38., 38.,\n",
      "       38., 38., 38., 38., 38., 38., 38.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([30,  3,  3, 14,  3,  3, 16,  3,  3, 16,  3,  3,  2]) acao=4 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=4 prox_estado=array([30,  3,  4, 13,  3,  4, 15,  3,  4, 15,  3,  4,  3]) recompensa=36.5 terminado=False\n",
      "objetivo_td=36.5 erro_td=array([36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5,\n",
      "       36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([30,  3,  4, 13,  3,  4, 15,  3,  4, 15,  3,  4,  3]) acao=5 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=5 prox_estado=array([17,  4,  5, 11,  4,  5, 13,  4,  5, 13,  4,  5, 16]) recompensa=27.0 terminado=False\n",
      "objetivo_td=27.0 erro_td=array([27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27.,\n",
      "       27., 27., 27., 27., 27., 27., 27.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([17,  4,  5, 11,  4,  5, 13,  4,  5, 13,  4,  5, 16]) acao=5 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=5 prox_estado=array([ 1,  5,  5, 10,  5,  5, 12,  5,  5, 12,  5,  5, 20]) recompensa=17.5 terminado=False\n",
      "objetivo_td=17.5 erro_td=array([17.5, 17.5, 17.5, 17.5, 17.5, 17.5, 17.5, 17.5, 17.5, 17.5, 17.5,\n",
      "       17.5, 17.5, 17.5, 17.5, 17.5, 17.5, 17.5, 17.5, 17.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 1,  5,  5, 10,  5,  5, 12,  5,  5, 12,  5,  5, 20]) acao=0 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=0 prox_estado=array([ 0,  5,  0, 15,  5,  0, 17,  5,  0, 17,  5,  0,  8]) recompensa=26.5 terminado=False\n",
      "objetivo_td=26.5 erro_td=array([26.5, 26.5, 26.5, 26.5, 26.5, 26.5, 26.5, 26.5, 26.5, 26.5, 26.5,\n",
      "       26.5, 26.5, 26.5, 26.5, 26.5, 26.5, 26.5, 26.5, 26.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  5,  0, 15,  5,  0, 17,  5,  0, 17,  5,  0,  8]) acao=9 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=9 prox_estado=array([ 0,  0,  9, 11,  0,  9, 13,  0,  9, 13,  0,  9,  6]) recompensa=19.5 terminado=False\n",
      "objetivo_td=19.5 erro_td=array([19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5,\n",
      "       19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  0,  9, 11,  0,  9, 13,  0,  9, 13,  0,  9,  6]) acao=1 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=1 prox_estado=array([ 0,  9,  1, 10,  9,  1, 12,  9,  1, 12,  9,  1, 16]) recompensa=33.0 terminado=False\n",
      "objetivo_td=33.0 erro_td=array([33., 33., 33., 33., 33., 33., 33., 33., 33., 33., 33., 33., 33.,\n",
      "       33., 33., 33., 33., 33., 33., 33.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  9,  1, 10,  9,  1, 12,  9,  1, 12,  9,  1, 16]) acao=11 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=11 prox_estado=array([ 0,  1, 11,  8,  1, 11, 10,  1, 11, 10,  1, 11, 12]) recompensa=17.0 terminado=False\n",
      "objetivo_td=17.0 erro_td=array([17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
      "       17., 17., 17., 17., 17., 17., 17.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  1, 11,  8,  1, 11, 10,  1, 11, 10,  1, 11, 12]) acao=1 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=1 prox_estado=array([ 0, 11,  1,  8, 11,  1, 10, 11,  1, 10, 11,  1,  9]) recompensa=22.0 terminado=False\n",
      "objetivo_td=22.0 erro_td=array([22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22.,\n",
      "       22., 22., 22., 22., 22., 22., 22.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0, 11,  1,  8, 11,  1, 10, 11,  1, 10, 11,  1,  9]) acao=2 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=2 prox_estado=array([ 9,  1,  2, 17,  1,  2, 19,  1,  2, 19,  1,  2,  2]) recompensa=32.0 terminado=False\n",
      "objetivo_td=32.0 erro_td=array([32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
      "       32., 32., 32., 32., 32., 32., 32.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 9,  1,  2, 17,  1,  2, 19,  1,  2, 19,  1,  2,  2]) acao=1 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=1 prox_estado=array([ 0,  2,  1, 17,  2,  1, 19,  2,  1, 19,  2,  1, 14]) recompensa=31.5 terminado=False\n",
      "objetivo_td=31.5 erro_td=array([31.5, 31.5, 31.5, 31.5, 31.5, 31.5, 31.5, 31.5, 31.5, 31.5, 31.5,\n",
      "       31.5, 31.5, 31.5, 31.5, 31.5, 31.5, 31.5, 31.5, 31.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  2,  1, 17,  2,  1, 19,  2,  1, 19,  2,  1, 14]) acao=2 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=2 prox_estado=array([ 0,  1,  2, 17,  1,  2, 19,  1,  2, 19,  1,  2, 11]) recompensa=36.5 terminado=False\n",
      "objetivo_td=36.5 erro_td=array([36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5,\n",
      "       36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5, 36.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  1,  2, 17,  1,  2, 19,  1,  2, 19,  1,  2, 11]) acao=6 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=6 prox_estado=array([ 0,  2,  6, 12,  2,  6, 14,  2,  6, 14,  2,  6,  3]) recompensa=22.0 terminado=False\n",
      "objetivo_td=22.0 erro_td=array([22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22.,\n",
      "       22., 22., 22., 22., 22., 22., 22.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  2,  6, 12,  2,  6, 14,  2,  6, 14,  2,  6,  3]) acao=9 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=9 prox_estado=array([0, 6, 9, 5, 6, 9, 7, 6, 9, 7, 6, 9, 6]) recompensa=13.5 terminado=False\n",
      "objetivo_td=13.5 erro_td=array([13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5,\n",
      "       13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([0, 6, 9, 5, 6, 9, 7, 6, 9, 7, 6, 9, 6]) acao=12 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=12 prox_estado=array([ 0,  9, 11,  0,  9, 12,  1,  9, 12,  1,  9, 12,  6]) recompensa=2.0 terminado=False\n",
      "objetivo_td=2.0 erro_td=array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "       2., 2., 2.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  9, 11,  0,  9, 12,  1,  9, 12,  1,  9, 12,  6]) acao=2 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=2 prox_estado=array([ 2, 11,  2,  7, 12,  2,  8, 12,  2,  8, 12,  2,  7]) recompensa=12.5 terminado=False\n",
      "objetivo_td=12.5 erro_td=array([12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5,\n",
      "       12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 2, 11,  2,  7, 12,  2,  8, 12,  2,  8, 12,  2,  7]) acao=11 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=11 prox_estado=array([ 0,  2, 11,  8,  2, 11,  9,  2, 11,  9,  2, 11, 13]) recompensa=13.0 terminado=False\n",
      "objetivo_td=13.0 erro_td=array([13., 13., 13., 13., 13., 13., 13., 13., 13., 13., 13., 13., 13.,\n",
      "       13., 13., 13., 13., 13., 13., 13.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  2, 11,  8,  2, 11,  9,  2, 11,  9,  2, 11, 13]) acao=12 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=12 prox_estado=array([ 0, 11, 10,  0, 11, 11,  0, 11, 11,  0, 11, 12,  6]) recompensa=8.0 terminado=False\n",
      "objetivo_td=8.0 erro_td=array([8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8., 8.,\n",
      "       8., 8., 8.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0, 11, 10,  0, 11, 11,  0, 11, 11,  0, 11, 12,  6]) acao=3 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=3 prox_estado=array([10, 10,  3,  8, 11,  3,  8, 11,  3,  8, 12,  3,  1]) recompensa=17.0 terminado=False\n",
      "objetivo_td=17.0 erro_td=array([17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
      "       17., 17., 17., 17., 17., 17., 17.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([10, 10,  3,  8, 11,  3,  8, 11,  3,  8, 12,  3,  1]) acao=7 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=7 prox_estado=array([ 2,  3,  7, 12,  3,  7, 12,  3,  7, 13,  3,  7, 18]) recompensa=19.5 terminado=False\n",
      "objetivo_td=19.5 erro_td=array([19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5,\n",
      "       19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 2,  3,  7, 12,  3,  7, 12,  3,  7, 13,  3,  7, 18]) acao=1 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=1 prox_estado=array([ 0,  7,  1, 14,  7,  1, 14,  7,  1, 15,  7,  1, 16]) recompensa=32.5 terminado=False\n",
      "objetivo_td=32.5 erro_td=array([32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5,\n",
      "       32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  7,  1, 14,  7,  1, 14,  7,  1, 15,  7,  1, 16]) acao=8 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=8 prox_estado=array([ 0,  1,  8, 13,  1,  8, 13,  1,  8, 14,  1,  8,  9]) recompensa=22.0 terminado=False\n",
      "objetivo_td=22.0 erro_td=array([22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22.,\n",
      "       22., 22., 22., 22., 22., 22., 22.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  1,  8, 13,  1,  8, 13,  1,  8, 14,  1,  8,  9]) acao=7 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=7 prox_estado=array([1, 8, 7, 7, 8, 7, 7, 8, 7, 8, 8, 7, 0]) recompensa=11.5 terminado=False\n",
      "objetivo_td=11.5 erro_td=array([11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5,\n",
      "       11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([1, 8, 7, 7, 8, 7, 7, 8, 7, 8, 8, 7, 0]) acao=10 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=10 prox_estado=array([ 0,  7, 10,  5,  7, 10,  5,  7, 10,  6,  7, 10, 16]) recompensa=15.0 terminado=False\n",
      "objetivo_td=15.0 erro_td=array([15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15.,\n",
      "       15., 15., 15., 15., 15., 15., 15.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  7, 10,  5,  7, 10,  5,  7, 10,  6,  7, 10, 16]) acao=8 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=8 prox_estado=array([ 0, 10,  8,  4, 10,  8,  4, 10,  8,  5, 10,  8, 11]) recompensa=10.5 terminado=False\n",
      "objetivo_td=10.5 erro_td=array([10.5, 10.5, 10.5, 10.5, 10.5, 10.5, 10.5, 10.5, 10.5, 10.5, 10.5,\n",
      "       10.5, 10.5, 10.5, 10.5, 10.5, 10.5, 10.5, 10.5, 10.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0, 10,  8,  4, 10,  8,  4, 10,  8,  5, 10,  8, 11]) acao=0 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=0 prox_estado=array([ 7,  8,  0, 14,  8,  0, 14,  8,  0, 15,  8,  0,  3]) recompensa=25.0 terminado=False\n",
      "objetivo_td=25.0 erro_td=array([25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25.,\n",
      "       25., 25., 25., 25., 25., 25., 25.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 7,  8,  0, 14,  8,  0, 14,  8,  0, 15,  8,  0,  3]) acao=0 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=0 prox_estado=array([ 0,  0,  0, 22,  0,  0, 22,  0,  0, 23,  0,  0, 20]) recompensa=38.5 terminado=False\n",
      "objetivo_td=38.5 erro_td=array([38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5,\n",
      "       38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5, 38.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  0,  0, 22,  0,  0, 22,  0,  0, 23,  0,  0, 20]) acao=2 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=2 prox_estado=array([ 0,  0,  2, 20,  0,  2, 20,  0,  2, 21,  0,  2, 11]) recompensa=41.5 terminado=False\n",
      "objetivo_td=41.5 erro_td=array([41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5,\n",
      "       41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  0,  2, 20,  0,  2, 20,  0,  2, 21,  0,  2, 11]) acao=6 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=6 prox_estado=array([ 0,  2,  6, 14,  2,  6, 14,  2,  6, 15,  2,  6, 20]) recompensa=41.5 terminado=False\n",
      "objetivo_td=41.5 erro_td=array([41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5,\n",
      "       41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5, 41.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  2,  6, 14,  2,  6, 14,  2,  6, 15,  2,  6, 20]) acao=6 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=6 prox_estado=array([ 0,  6,  6, 10,  6,  6, 10,  6,  6, 11,  6,  6,  3]) recompensa=16.5 terminado=False\n",
      "objetivo_td=16.5 erro_td=array([16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5,\n",
      "       16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5, 16.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  6,  6, 10,  6,  6, 10,  6,  6, 11,  6,  6,  3]) acao=4 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=4 prox_estado=array([ 0,  6,  4, 12,  6,  4, 12,  6,  4, 13,  6,  4,  6]) recompensa=18.5 terminado=False\n",
      "objetivo_td=18.5 erro_td=array([18.5, 18.5, 18.5, 18.5, 18.5, 18.5, 18.5, 18.5, 18.5, 18.5, 18.5,\n",
      "       18.5, 18.5, 18.5, 18.5, 18.5, 18.5, 18.5, 18.5, 18.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  6,  4, 12,  6,  4, 12,  6,  4, 13,  6,  4,  6]) acao=6 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=6 prox_estado=array([ 4,  4,  6, 12,  4,  6, 12,  4,  6, 13,  4,  6,  2]) recompensa=20.5 terminado=True\n",
      "objetivo_td=20.5 erro_td=array([20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5,\n",
      "       20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "e=1\n",
      "terminado=False\n",
      "ANTES: estado=array([12,  4,  4, 12,  4,  4, 12,  4,  4, 12,  4,  4,  2]) acao=5 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=5 prox_estado=array([15,  4,  5, 11,  4,  5, 11,  4,  5, 11,  4,  5,  1]) recompensa=24.0 terminado=False\n",
      "objetivo_td=24.0 erro_td=array([24., 24., 24., 24., 24., 24., 24., 24., 24., 24., 24., 24., 24.,\n",
      "       24., 24., 24., 24., 24., 24., 24.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([15,  4,  5, 11,  4,  5, 11,  4,  5, 11,  4,  5,  1]) acao=2 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=2 prox_estado=array([ 2,  5,  2, 13,  5,  2, 13,  5,  2, 13,  5,  2, 17]) recompensa=20.5 terminado=False\n",
      "objetivo_td=20.5 erro_td=array([20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5,\n",
      "       20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 2,  5,  2, 13,  5,  2, 13,  5,  2, 13,  5,  2, 17]) acao=3 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=3 prox_estado=array([ 0,  2,  3, 15,  2,  3, 15,  2,  3, 15,  2,  3, 10]) recompensa=25.5 terminado=False\n",
      "objetivo_td=25.5 erro_td=array([25.5, 25.5, 25.5, 25.5, 25.5, 25.5, 25.5, 25.5, 25.5, 25.5, 25.5,\n",
      "       25.5, 25.5, 25.5, 25.5, 25.5, 25.5, 25.5, 25.5, 25.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  2,  3, 15,  2,  3, 15,  2,  3, 15,  2,  3, 10]) acao=9 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=9 prox_estado=array([ 0,  3,  9,  8,  3,  9,  8,  3,  9,  8,  3,  9, 12]) recompensa=22.0 terminado=False\n",
      "objetivo_td=22.0 erro_td=array([22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22.,\n",
      "       22., 22., 22., 22., 22., 22., 22.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  3,  9,  8,  3,  9,  8,  3,  9,  8,  3,  9, 12]) acao=10 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=10 prox_estado=array([ 0,  9, 10,  1,  9, 10,  1,  9, 10,  1,  9, 10,  3]) recompensa=1.5 terminado=False\n",
      "objetivo_td=1.5 erro_td=array([1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5,\n",
      "       1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  9, 10,  1,  9, 10,  1,  9, 10,  1,  9, 10,  3]) acao=8 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=8 prox_estado=array([ 0, 10,  8,  2, 10,  8,  2, 10,  8,  2, 10,  8, 10]) recompensa=4.0 terminado=False\n",
      "objetivo_td=4.0 erro_td=array([4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "       4., 4., 4.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0, 10,  8,  2, 10,  8,  2, 10,  8,  2, 10,  8, 10]) acao=6 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=6 prox_estado=array([ 0,  8,  6,  6,  8,  6,  6,  8,  6,  6,  8,  6, 14]) recompensa=13.0 terminado=False\n",
      "objetivo_td=13.0 erro_td=array([13., 13., 13., 13., 13., 13., 13., 13., 13., 13., 13., 13., 13.,\n",
      "       13., 13., 13., 13., 13., 13., 13.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  8,  6,  6,  8,  6,  6,  8,  6,  6,  8,  6, 14]) acao=0 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=0 prox_estado=array([ 0,  6,  0, 14,  6,  0, 14,  6,  0, 14,  6,  0, 17]) recompensa=30.0 terminado=False\n",
      "objetivo_td=30.0 erro_td=array([30., 30., 30., 30., 30., 30., 30., 30., 30., 30., 30., 30., 30.,\n",
      "       30., 30., 30., 30., 30., 30., 30.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  6,  0, 14,  6,  0, 14,  6,  0, 14,  6,  0, 17]) acao=5 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=5 prox_estado=array([ 0,  0,  5, 15,  0,  5, 15,  0,  5, 15,  0,  5, 13]) recompensa=29.5 terminado=False\n",
      "objetivo_td=29.5 erro_td=array([29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5,\n",
      "       29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  0,  5, 15,  0,  5, 15,  0,  5, 15,  0,  5, 13]) acao=7 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=7 prox_estado=array([0, 5, 7, 8, 5, 7, 8, 5, 7, 8, 5, 7, 7]) recompensa=19.0 terminado=False\n",
      "objetivo_td=19.0 erro_td=array([19., 19., 19., 19., 19., 19., 19., 19., 19., 19., 19., 19., 19.,\n",
      "       19., 19., 19., 19., 19., 19., 19.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([0, 5, 7, 8, 5, 7, 8, 5, 7, 8, 5, 7, 7]) acao=7 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=7 prox_estado=array([ 0,  7,  7,  6,  7,  7,  6,  7,  7,  6,  7,  7, 14]) recompensa=18.0 terminado=False\n",
      "objetivo_td=18.0 erro_td=array([18., 18., 18., 18., 18., 18., 18., 18., 18., 18., 18., 18., 18.,\n",
      "       18., 18., 18., 18., 18., 18., 18.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  7,  7,  6,  7,  7,  6,  7,  7,  6,  7,  7, 14]) acao=2 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=2 prox_estado=array([ 0,  7,  2, 11,  7,  2, 11,  7,  2, 11,  7,  2, 10]) recompensa=19.5 terminado=False\n",
      "objetivo_td=19.5 erro_td=array([19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5,\n",
      "       19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5, 19.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  7,  2, 11,  7,  2, 11,  7,  2, 11,  7,  2, 10]) acao=7 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=7 prox_estado=array([ 1,  2,  7, 11,  2,  7, 11,  2,  7, 11,  2,  7,  6]) recompensa=17.0 terminado=False\n",
      "objetivo_td=17.0 erro_td=array([17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
      "       17., 17., 17., 17., 17., 17., 17.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 1,  2,  7, 11,  2,  7, 11,  2,  7, 11,  2,  7,  6]) acao=0 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=0 prox_estado=array([ 0,  7,  0, 13,  7,  0, 13,  7,  0, 13,  7,  0,  9]) recompensa=25.5 terminado=False\n",
      "objetivo_td=25.5 erro_td=array([25.5, 25.5, 25.5, 25.5, 25.5, 25.5, 25.5, 25.5, 25.5, 25.5, 25.5,\n",
      "       25.5, 25.5, 25.5, 25.5, 25.5, 25.5, 25.5, 25.5, 25.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  7,  0, 13,  7,  0, 13,  7,  0, 13,  7,  0,  9]) acao=9 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=9 prox_estado=array([ 0,  0,  9, 11,  0,  9, 11,  0,  9, 11,  0,  9, 13]) recompensa=22.5 terminado=False\n",
      "objetivo_td=22.5 erro_td=array([22.5, 22.5, 22.5, 22.5, 22.5, 22.5, 22.5, 22.5, 22.5, 22.5, 22.5,\n",
      "       22.5, 22.5, 22.5, 22.5, 22.5, 22.5, 22.5, 22.5, 22.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  0,  9, 11,  0,  9, 11,  0,  9, 11,  0,  9, 13]) acao=4 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=4 prox_estado=array([0, 9, 4, 7, 9, 4, 7, 9, 4, 7, 9, 4, 1]) recompensa=11.5 terminado=False\n",
      "objetivo_td=11.5 erro_td=array([11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5,\n",
      "       11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5, 11.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([0, 9, 4, 7, 9, 4, 7, 9, 4, 7, 9, 4, 1]) acao=0 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=0 prox_estado=array([ 5,  4,  0, 16,  4,  0, 16,  4,  0, 16,  4,  0,  4]) recompensa=26.5 terminado=False\n",
      "objetivo_td=26.5 erro_td=array([26.5, 26.5, 26.5, 26.5, 26.5, 26.5, 26.5, 26.5, 26.5, 26.5, 26.5,\n",
      "       26.5, 26.5, 26.5, 26.5, 26.5, 26.5, 26.5, 26.5, 26.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 5,  4,  0, 16,  4,  0, 16,  4,  0, 16,  4,  0,  4]) acao=6 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=6 prox_estado=array([ 1,  0,  6, 14,  0,  6, 14,  0,  6, 14,  0,  6,  8]) recompensa=21.5 terminado=False\n",
      "objetivo_td=21.5 erro_td=array([21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5,\n",
      "       21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 1,  0,  6, 14,  0,  6, 14,  0,  6, 14,  0,  6,  8]) acao=0 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=0 prox_estado=array([ 0,  6,  0, 14,  6,  0, 14,  6,  0, 14,  6,  0, 12]) recompensa=32.0 terminado=False\n",
      "objetivo_td=32.0 erro_td=array([32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
      "       32., 32., 32., 32., 32., 32., 32.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  6,  0, 14,  6,  0, 14,  6,  0, 14,  6,  0, 12]) acao=1 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=1 prox_estado=array([ 1,  0,  1, 19,  0,  1, 19,  0,  1, 19,  0,  1,  5]) recompensa=29.0 terminado=False\n",
      "objetivo_td=29.0 erro_td=array([29., 29., 29., 29., 29., 29., 29., 29., 29., 29., 29., 29., 29.,\n",
      "       29., 29., 29., 29., 29., 29., 29.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 1,  0,  1, 19,  0,  1, 19,  0,  1, 19,  0,  1,  5]) acao=9 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=9 prox_estado=array([ 0,  1,  9, 10,  1,  9, 10,  1,  9, 10,  1,  9,  3]) recompensa=17.0 terminado=False\n",
      "objetivo_td=17.0 erro_td=array([17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
      "       17., 17., 17., 17., 17., 17., 17.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  1,  9, 10,  1,  9, 10,  1,  9, 10,  1,  9,  3]) acao=6 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=6 prox_estado=array([ 0,  9,  6,  5,  9,  6,  5,  9,  6,  5,  9,  6, 15]) recompensa=21.5 terminado=False\n",
      "objetivo_td=21.5 erro_td=array([21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5,\n",
      "       21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  9,  6,  5,  9,  6,  5,  9,  6,  5,  9,  6, 15]) acao=8 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=8 prox_estado=array([ 0,  6,  8,  6,  6,  8,  6,  6,  8,  6,  6,  8, 18]) recompensa=18.0 terminado=False\n",
      "objetivo_td=18.0 erro_td=array([18., 18., 18., 18., 18., 18., 18., 18., 18., 18., 18., 18., 18.,\n",
      "       18., 18., 18., 18., 18., 18., 18.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  6,  8,  6,  6,  8,  6,  6,  8,  6,  6,  8, 18]) acao=8 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=8 prox_estado=array([ 0,  8,  8,  4,  8,  8,  4,  8,  8,  4,  8,  8, 19]) recompensa=19.0 terminado=False\n",
      "objetivo_td=19.0 erro_td=array([19., 19., 19., 19., 19., 19., 19., 19., 19., 19., 19., 19., 19.,\n",
      "       19., 19., 19., 19., 19., 19., 19.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  8,  8,  4,  8,  8,  4,  8,  8,  4,  8,  8, 19]) acao=11 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=11 prox_estado=array([ 0,  8, 11,  1,  8, 11,  1,  8, 11,  1,  8, 11, 19]) recompensa=12.5 terminado=False\n",
      "objetivo_td=12.5 erro_td=array([12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5,\n",
      "       12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  8, 11,  1,  8, 11,  1,  8, 11,  1,  8, 11, 19]) acao=7 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=7 prox_estado=array([ 0, 11,  7,  2, 11,  7,  2, 11,  7,  2, 11,  7, 16]) recompensa=11.0 terminado=False\n",
      "objetivo_td=11.0 erro_td=array([11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
      "       11., 11., 11., 11., 11., 11., 11.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0, 11,  7,  2, 11,  7,  2, 11,  7,  2, 11,  7, 16]) acao=4 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=4 prox_estado=array([ 0,  7,  4,  9,  7,  4,  9,  7,  4,  9,  7,  4, 11]) recompensa=13.5 terminado=False\n",
      "objetivo_td=13.5 erro_td=array([13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5,\n",
      "       13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  7,  4,  9,  7,  4,  9,  7,  4,  9,  7,  4, 11]) acao=3 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=3 prox_estado=array([ 1,  4,  3, 13,  4,  3, 13,  4,  3, 13,  4,  3,  6]) recompensa=20.0 terminado=False\n",
      "objetivo_td=20.0 erro_td=array([20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
      "       20., 20., 20., 20., 20., 20., 20.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 1,  4,  3, 13,  4,  3, 13,  4,  3, 13,  4,  3,  6]) acao=7 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=7 prox_estado=array([ 2,  3,  7, 10,  3,  7, 10,  3,  7, 10,  3,  7,  3]) recompensa=16.0 terminado=False\n",
      "objetivo_td=16.0 erro_td=array([16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
      "       16., 16., 16., 16., 16., 16., 16.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 2,  3,  7, 10,  3,  7, 10,  3,  7, 10,  3,  7,  3]) acao=2 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=2 prox_estado=array([ 0,  7,  2, 11,  7,  2, 11,  7,  2, 11,  7,  2, 16]) recompensa=27.5 terminado=False\n",
      "objetivo_td=27.5 erro_td=array([27.5, 27.5, 27.5, 27.5, 27.5, 27.5, 27.5, 27.5, 27.5, 27.5, 27.5,\n",
      "       27.5, 27.5, 27.5, 27.5, 27.5, 27.5, 27.5, 27.5, 27.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  7,  2, 11,  7,  2, 11,  7,  2, 11,  7,  2, 16]) acao=10 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=10 prox_estado=array([ 0,  2, 10,  8,  2, 10,  8,  2, 10,  8,  2, 10,  9]) recompensa=14.0 terminado=False\n",
      "objetivo_td=14.0 erro_td=array([14., 14., 14., 14., 14., 14., 14., 14., 14., 14., 14., 14., 14.,\n",
      "       14., 14., 14., 14., 14., 14., 14.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  2, 10,  8,  2, 10,  8,  2, 10,  8,  2, 10,  9]) acao=7 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=7 prox_estado=array([ 0, 10,  7,  3, 10,  7,  3, 10,  7,  3, 10,  7, 12]) recompensa=14.5 terminado=False\n",
      "objetivo_td=14.5 erro_td=array([14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5,\n",
      "       14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0, 10,  7,  3, 10,  7,  3, 10,  7,  3, 10,  7, 12]) acao=0 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=0 prox_estado=array([ 0,  7,  0, 13,  7,  0, 13,  7,  0, 13,  7,  0, 15]) recompensa=24.5 terminado=False\n",
      "objetivo_td=24.5 erro_td=array([24.5, 24.5, 24.5, 24.5, 24.5, 24.5, 24.5, 24.5, 24.5, 24.5, 24.5,\n",
      "       24.5, 24.5, 24.5, 24.5, 24.5, 24.5, 24.5, 24.5, 24.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  7,  0, 13,  7,  0, 13,  7,  0, 13,  7,  0, 15]) acao=12 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=12 prox_estado=array([ 6,  0, 12,  8,  0, 12,  8,  0, 12,  8,  0, 12,  1]) recompensa=15.0 terminado=False\n",
      "objetivo_td=15.0 erro_td=array([15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15.,\n",
      "       15., 15., 15., 15., 15., 15., 15.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 6,  0, 12,  8,  0, 12,  8,  0, 12,  8,  0, 12,  1]) acao=12 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=12 prox_estado=array([ 4, 12,  8,  0, 12,  8,  0, 12,  8,  0, 12, 12,  2]) recompensa=14.0 terminado=False\n",
      "objetivo_td=14.0 erro_td=array([14., 14., 14., 14., 14., 14., 14., 14., 14., 14., 14., 14., 14.,\n",
      "       14., 14., 14., 14., 14., 14., 14.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 4, 12,  8,  0, 12,  8,  0, 12,  8,  0, 12, 12,  2]) acao=0 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=0 prox_estado=array([ 5,  8,  0, 12,  8,  0, 12,  8,  0, 12, 12,  0, 11]) recompensa=20.5 terminado=False\n",
      "objetivo_td=20.5 erro_td=array([20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5,\n",
      "       20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5, 20.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 5,  8,  0, 12,  8,  0, 12,  8,  0, 12, 12,  0, 11]) acao=4 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=4 prox_estado=array([ 0,  0,  4, 16,  0,  4, 16,  0,  4, 20,  0,  4, 16]) recompensa=29.0 terminado=False\n",
      "objetivo_td=29.0 erro_td=array([29., 29., 29., 29., 29., 29., 29., 29., 29., 29., 29., 29., 29.,\n",
      "       29., 29., 29., 29., 29., 29., 29.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  0,  4, 16,  0,  4, 16,  0,  4, 20,  0,  4, 16]) acao=3 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=3 prox_estado=array([ 0,  4,  3, 13,  4,  3, 13,  4,  3, 17,  4,  3,  8]) recompensa=29.5 terminado=False\n",
      "objetivo_td=29.5 erro_td=array([29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5,\n",
      "       29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5, 29.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  4,  3, 13,  4,  3, 13,  4,  3, 17,  4,  3,  8]) acao=9 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=9 prox_estado=array([ 0,  3,  9,  8,  3,  9,  8,  3,  9, 12,  3,  9, 11]) recompensa=21.0 terminado=False\n",
      "objetivo_td=21.0 erro_td=array([21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21.,\n",
      "       21., 21., 21., 21., 21., 21., 21.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  3,  9,  8,  3,  9,  8,  3,  9, 12,  3,  9, 11]) acao=5 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=5 prox_estado=array([ 0,  9,  5,  6,  9,  5,  6,  9,  5, 10,  9,  5, 20]) recompensa=28.0 terminado=False\n",
      "objetivo_td=28.0 erro_td=array([28., 28., 28., 28., 28., 28., 28., 28., 28., 28., 28., 28., 28.,\n",
      "       28., 28., 28., 28., 28., 28., 28.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  9,  5,  6,  9,  5,  6,  9,  5, 10,  9,  5, 20]) acao=2 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=2 prox_estado=array([ 4,  5,  2, 13,  5,  2, 13,  5,  2, 17,  5,  2,  5]) recompensa=23.5 terminado=False\n",
      "objetivo_td=23.5 erro_td=array([23.5, 23.5, 23.5, 23.5, 23.5, 23.5, 23.5, 23.5, 23.5, 23.5, 23.5,\n",
      "       23.5, 23.5, 23.5, 23.5, 23.5, 23.5, 23.5, 23.5, 23.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 4,  5,  2, 13,  5,  2, 13,  5,  2, 17,  5,  2,  5]) acao=11 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=11 prox_estado=array([ 0,  2, 11,  7,  2, 11,  7,  2, 11, 11,  2, 11, 18]) recompensa=21.5 terminado=False\n",
      "objetivo_td=21.5 erro_td=array([21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5,\n",
      "       21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5, 21.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  2, 11,  7,  2, 11,  7,  2, 11, 11,  2, 11, 18]) acao=11 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=11 prox_estado=array([ 0, 11,  9,  0, 11,  9,  0, 11, 11,  2, 11, 11, 13]) recompensa=16.0 terminado=False\n",
      "objetivo_td=16.0 erro_td=array([16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
      "       16., 16., 16., 16., 16., 16., 16.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0, 11,  9,  0, 11,  9,  0, 11, 11,  2, 11, 11, 13]) acao=1 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=1 prox_estado=array([ 0,  9,  1, 10,  9,  1, 10, 11,  1, 12, 11,  1, 16]) recompensa=21.0 terminado=False\n",
      "objetivo_td=21.0 erro_td=array([21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21.,\n",
      "       21., 21., 21., 21., 21., 21., 21.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  9,  1, 10,  9,  1, 10, 11,  1, 12, 11,  1, 16]) acao=11 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=11 prox_estado=array([ 4,  1, 11,  8,  1, 11, 10,  1, 11, 12,  1, 11,  5]) recompensa=17.0 terminado=False\n",
      "objetivo_td=17.0 erro_td=array([17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
      "       17., 17., 17., 17., 17., 17., 17.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 4,  1, 11,  8,  1, 11, 10,  1, 11, 12,  1, 11,  5]) acao=3 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=3 prox_estado=array([ 0, 11,  3,  6, 11,  3,  8, 11,  3, 10, 11,  3, 18]) recompensa=25.0 terminado=False\n",
      "objetivo_td=25.0 erro_td=array([25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25., 25.,\n",
      "       25., 25., 25., 25., 25., 25., 25.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0, 11,  3,  6, 11,  3,  8, 11,  3, 10, 11,  3, 18]) acao=12 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=12 prox_estado=array([ 0,  3, 12,  5,  3, 12,  7,  3, 12,  9,  3, 12, 14]) recompensa=13.5 terminado=False\n",
      "objetivo_td=13.5 erro_td=array([13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5,\n",
      "       13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5, 13.5]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  3, 12,  5,  3, 12,  7,  3, 12,  9,  3, 12, 14]) acao=2 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=2 prox_estado=array([ 0, 12,  2,  6, 12,  2,  8, 12,  2, 10, 12,  2,  8]) recompensa=17.0 terminado=False\n",
      "objetivo_td=17.0 erro_td=array([17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
      "       17., 17., 17., 17., 17., 17., 17.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0, 12,  2,  6, 12,  2,  8, 12,  2, 10, 12,  2,  8]) acao=12 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=12 prox_estado=array([ 0,  2, 12,  6,  2, 12,  8,  2, 12, 10,  2, 12, 16]) recompensa=16.0 terminado=False\n",
      "objetivo_td=16.0 erro_td=array([16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
      "       16., 16., 16., 16., 16., 16., 16.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "ANTES: estado=array([ 0,  2, 12,  6,  2, 12,  8,  2, 12, 10,  2, 12, 16]) acao=0 Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "acao=0 prox_estado=array([ 0, 12,  0,  8, 12,  0, 10, 12,  0, 12, 12,  0, 11]) recompensa=24.0 terminado=True\n",
      "objetivo_td=24.0 erro_td=array([24., 24., 24., 24., 24., 24., 24., 24., 24., 24., 24., 24., 24.,\n",
      "       24., 24., 24., 24., 24., 24., 24.]) Q[estado][acao]=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0.])\n",
      "Q = [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "V = [0. 0. 0. ... 0. 0. 0.]\n",
      "pi = <function q_learning.<locals>.<lambda> at 0x000001D29543AB90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "beer_game : BeerGameSimplificado = BeerGameSimplificado(seed=10)\n",
    "\n",
    "estado : list[int] = beer_game.reset()\n",
    "print(f'estado inicial {estado}')\n",
    "\n",
    "\n",
    "Q, V, pi, Q_historico, pi_historico = q_learning(beer_game, n_episodios=2)\n",
    "\n",
    "print(f'Q = {Q}')\n",
    "print(f'V = {V}')\n",
    "print(f'pi = {pi}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
